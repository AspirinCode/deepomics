{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics import utils, learn\n",
    "\n",
    "# import models\n",
    "from model_zoo import fourthplace_connectomics_model\n",
    "from model_zoo import simple_connectomics_model, simple_connectomics_model2\n",
    "from model_zoo import residual_connectomics_model, residual_connectomics_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 432 ms, sys: 1.06 s, total: 1.49 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "#data_path = 'D:/Dropbox/TFconnect'\n",
    "data_path = '/home/peter/Code/tensorflow/data'\n",
    "filepath = os.path.join(data_path,filename)\n",
    "\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(filepath,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "X_train = dtf#[:10000,:,:,:]\n",
    "y_train = ltf#[:10000,:]\n",
    "X_valid = dtf_crossval#[:5000,:,:,:]\n",
    "y_valid = ltf_crossval#[:5000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fourth place competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = fourthplace_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = 'fourth_place'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=100, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple connectomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = simple_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=200, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple connectomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = simple_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version2'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=100, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=100, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# residual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = 'residual'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=500, num_epochs=100, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=2000, num_epochs=100, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# residual model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Network architecture:\n",
      "----------------------------------------------------------------------------\n",
      "layer1: input\n",
      "(?, 3, 330, 1)\n",
      "layer2: conv1\n",
      "(?, 2, 326, 32)\n",
      "layer3: conv1_batch\n",
      "(?, 2, 326, 32)\n",
      "layer4: conv1_active\n",
      "(?, 2, 326, 32)\n",
      "layer5: conv1_dropout\n",
      "(?, 2, 326, 32)\n",
      "layer6: resid1_1resid\n",
      "(?, 2, 326, 32)\n",
      "layer7: resid1_1resid_norm\n",
      "(?, 2, 326, 32)\n",
      "layer8: resid1_1resid_active\n",
      "(?, 2, 326, 32)\n",
      "layer9: resid1_dropout1\n",
      "(?, 2, 326, 32)\n",
      "layer10: resid1_2resid\n",
      "(?, 2, 326, 32)\n",
      "layer11: resid1_2resid_norm\n",
      "(?, 2, 326, 32)\n",
      "layer12: resid1_resid_sum\n",
      "(?, 2, 326, 32)\n",
      "layer13: resid1_resid\n",
      "(?, 2, 326, 32)\n",
      "layer14: resid1_dropout\n",
      "(?, 2, 326, 32)\n",
      "layer15: conv2\n",
      "(?, 1, 322, 64)\n",
      "layer16: conv2_batch\n",
      "(?, 1, 322, 64)\n",
      "layer17: conv2_active\n",
      "(?, 1, 322, 64)\n",
      "layer18: conv2_dropout\n",
      "(?, 1, 322, 64)\n",
      "layer19: resid2_1resid\n",
      "(?, 1, 322, 64)\n",
      "layer20: resid2_1resid_norm\n",
      "(?, 1, 322, 64)\n",
      "layer21: resid2_1resid_active\n",
      "(?, 1, 322, 64)\n",
      "layer22: resid2_dropout1\n",
      "(?, 1, 322, 64)\n",
      "layer23: resid2_2resid\n",
      "(?, 1, 322, 64)\n",
      "layer24: resid2_2resid_norm\n",
      "(?, 1, 322, 64)\n",
      "layer25: resid2_resid_sum\n",
      "(?, 1, 322, 64)\n",
      "layer26: resid2_resid\n",
      "(?, 1, 322, 64)\n",
      "layer27: resid2_pool\n",
      "(?, 1, 32, 64)\n",
      "layer28: resid2_dropout\n",
      "(?, 1, 32, 64)\n",
      "layer29: conv3\n",
      "(?, 1, 32, 128)\n",
      "layer30: conv3_batch\n",
      "(?, 1, 32, 128)\n",
      "layer31: conv3_active\n",
      "(?, 1, 32, 128)\n",
      "layer32: conv3_dropout\n",
      "(?, 1, 32, 128)\n",
      "layer33: dense1\n",
      "(?, 256)\n",
      "layer34: dense1_bias\n",
      "(?, 256)\n",
      "layer35: dense1_active\n",
      "(?, 256)\n",
      "layer36: dense1_dropout\n",
      "(?, 256)\n",
      "layer37: resid3_1resid\n",
      "(?, 256)\n",
      "layer38: resid3_1resid_norm\n",
      "(?, 256)\n",
      "layer39: resid3_1resid_active\n",
      "(?, 256)\n",
      "layer40: resid3_dropout1\n",
      "(?, 256)\n",
      "layer41: resid3_2resid\n",
      "(?, 256)\n",
      "layer42: resid3_2resid_norm\n",
      "(?, 256)\n",
      "layer43: resid3_resid_sum\n",
      "(?, 256)\n",
      "layer44: resid3_resid\n",
      "(?, 256)\n",
      "layer45: resid3_dropout\n",
      "(?, 256)\n",
      "layer46: dense2\n",
      "(?, 2)\n",
      "layer47: dense2_bias\n",
      "(?, 2)\n",
      "layer48: dense2_active\n",
      "(?, 2)\n",
      "layer49: output\n",
      "(?, 2)\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version_residual2'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 200 \n",
      "[==                            ] 5.9% -- time=292s -- loss=0.62148 -- accuracy=77.42%  "
     ]
    }
   ],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.9, 'keep_prob_dense': 0.7, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=200, \n",
    "                    patience=20, verbose=2, shuffle=True)\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=500, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1500, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=2000, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validation_score(nntrainer, data_path):\n",
    "\n",
    "    filename = 'valideval_dataset.hdf5'\n",
    "    dataset = h5py.File(os.path.join(data_path,filename),'r')\n",
    "    group_name = ['valid_data']\n",
    "    val_dat = np.array(dataset['/'+group_name[0]+'/vs_valid'])\n",
    "    val_lbl = np.array(dataset['/'+group_name[0]+'/label_valid'])\n",
    "\n",
    "    fragLen = 330\n",
    "    N = 14\n",
    "    avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "    startgap = np.ceil(float(val_dat.shape[1] - fragLen)/N).astype('int')\n",
    "    true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "\n",
    "    # Counter for the \"true_lbl\" array\n",
    "    cnt_ = 0\n",
    "    # Counter for the \"pred_lbl\" array\n",
    "    cnt_u = 0\n",
    "    for a in range(val_dat.shape[0]):\n",
    "        if a%100 == 0:\n",
    "            print('\\r' + 'X'*(a//100))\n",
    "\n",
    "        # Create batch array to send thru network\n",
    "        im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "\n",
    "        # Count the number of traces in each batch\n",
    "        cnt = 0\n",
    "\n",
    "        for b in range(val_dat.shape[0]):\n",
    "\n",
    "            for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "                try:\n",
    "                    im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                         val_dat[b,n:n+fragLen],\n",
    "                                         avg_F[n:n+fragLen]))\n",
    "                except:\n",
    "                    from IPython.core.debugger import Tracer\n",
    "                    Tracer()()\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "            # Keep track of the true labels\n",
    "            if val_lbl[a,b] == 1:\n",
    "                true_lbl[cnt_] = 1\n",
    "            else:\n",
    "                true_lbl[cnt_] = 0\n",
    "\n",
    "            cnt_ += 1\n",
    "\n",
    "        # Run batch through network\n",
    "        test = {'inputs': im_eval, 'keep_prob_conv': 1, 'keep_prob_dense': 1, 'is_training': False}\n",
    "        pred_stop = nntrainer.get_activations(test, layer='output')[:,0]\n",
    "        # Average output over each group of N traces\n",
    "        for u in range(0, len(pred_stop), N):\n",
    "            pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N])\n",
    "            cnt_u += 1        \n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "    fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "    wrk = auc(fpr, tpr)\n",
    "    print(wrk)\n",
    "    return wrk\n",
    "\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
