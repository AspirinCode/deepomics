{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tfomics import layers, utils, init, learn\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics.models import tim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(filename,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "X_train = dtf\n",
    "y_train = ltf\n",
    "X_valid = dtf_crossval\n",
    "y_valid = ltf_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "net, placeholders, optimization = tim_model.model(input_shape, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# design a neural network model\n",
    "input_vars = utils.placeholder(shape=input_shape, name='input')\n",
    "target_vars = utils.placeholder(shape=(None,num_labels), name='output')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')   # variable to specify training or testing\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')   # dropout probability\n",
    "placeholders = {'inputs': input_vars, \n",
    "                'targets': target_vars, \n",
    "                'keep_prob': keep_prob, \n",
    "                'is_training': is_training}\n",
    "#placeholders = [input_vars, target_vars, is_training, keep_prob] \n",
    "\n",
    "net = OrderedDict()\n",
    "net['inputs'] = layers.InputLayer(input_vars)\n",
    "net['conv1'] = layers.Conv2DLayer(net['inputs'], filter_size=(2,5), num_filters=18, W=init.TruncatedNormal(), padding='VALID')\n",
    "net['conv1_norm'] = layers.BatchNormLayer(net['conv1'], is_training)\n",
    "net['conv1_active'] = layers.ActivationLayer(net['conv1_norm'], function='relu')\n",
    "#net['conv1_dropout'] = layers.DropoutLayer(net['conv1_active'], keep_prob=keep_prob)\n",
    "\n",
    "net['conv2'] = layers.Conv2DLayer(net['conv1_active'], filter_size=(2,5), num_filters=40, W=init.TruncatedNormal(), \n",
    "                                  strides=(1,1), padding='VALID')\n",
    "net['conv2_norm'] = layers.BatchNormLayer(net['conv2'], is_training)\n",
    "net['conv2_active'] = layers.ActivationLayer(net['conv2_norm'], function='relu')\n",
    "net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(1,10))\n",
    "#net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], keep_prob=keep_prob)\n",
    "\n",
    "net['conv3'] = layers.Conv2DLayer(net['conv2_pool'], filter_size=(1,1), num_filters=15, W=init.TruncatedNormal(), \n",
    "                                  strides=(1,1), padding='VALID')\n",
    "net['conv3_norm'] = layers.BatchNormLayer(net['conv3'], is_training)\n",
    "net['conv3_active'] = layers.ActivationLayer(net['conv3_norm'], function='relu')\n",
    "#net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], keep_prob=keep_prob)\n",
    "\n",
    "net['dense1'] = layers.DenseLayer(net['conv3_active'], num_units=100, W=init.TruncatedNormal(), b=init.Constant(0.1))\n",
    "net['dense1_norm'] = layers.BatchNormLayer(net['dense1'], is_training)\n",
    "net['dense1_active'] = layers.ActivationLayer(net['dense1_norm'], function='relu')\n",
    "net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], keep_prob=keep_prob)\n",
    "\n",
    "net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, \n",
    "                                  W=init.TruncatedNormal(), b=init.Constant(0.1))\n",
    "net['output'] = layers.ActivationLayer(net['dense2'], function='sigmoid')\n",
    "\n",
    "optimization = {\"objective\": \"binary\",  # (binary, categorical, squared_error)\n",
    "                \"optimizer\": \"adam\",    \n",
    "                \"learning_rate\": 0.001, # learning rate\n",
    "                #\"clip_value\": True     # clip prediction values (True for classification)\n",
    "                #\"l2\": .00001            # l-2 weight decay\n",
    "                }\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, [placeholders['inputs']])\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, placeholders, optimization, save='best', filepath='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run session\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize variables\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(sess, nntrainer, data, batch_size=1000, num_epochs=500, \n",
    "                    patience=10, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
