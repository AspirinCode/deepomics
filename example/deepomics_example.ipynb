{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append('/home/peter/Code/deepomics')\n",
    "from neuralnetwork import NeuralNet, NeuralTrainer\n",
    "import train as fit \n",
    "import visualize, utils\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 324 ms, sys: 588 ms, total: 912 ms\n",
      "Wall time: 911 ms\n"
     ]
    }
   ],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "data_path = '/home/peter/Code/tensorflow/data'\n",
    "file_path = os.path.join(data_path, filename)\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(file_path,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "train = (dtf.transpose([0,3,1,2]), ltf)\n",
    "valid = (dtf_crossval.transpose([0,3,1,2]), ltf_crossval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n"
     ]
    }
   ],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=18, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=init.Constant(0.1), \n",
    "                                      nonlinearity=nonlinearities.tanh, pad='valid')\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1'], num_filters=40, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=init.Constant(0.1), \n",
    "                                      nonlinearity=nonlinearities.tanh, pad='valid')\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_pool'], num_filters=15, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=init.Constant(0.1), \n",
    "                                      nonlinearity=nonlinearities.tanh, pad='valid')\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3'], num_units=100, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=nonlinearities.rectify)\n",
    "    \n",
    "    net['output'] = layers.DenseLayer(net['dense1'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'test')\n",
    "output_name = 'original_model'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "savepath = utils.make_directory(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.40137 -- accuracy=83.96%  \n",
      "  valid loss:\t\t0.37519\n",
      "  valid accuracy:\t0.85309+/-0.00006\n",
      "  valid auc-roc:\t0.90830+/-0.00002\n",
      "  valid auc-pr:\t\t0.89853+/-0.00614\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 2 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.37267 -- accuracy=85.35%  \n",
      "  valid loss:\t\t0.36890\n",
      "  valid accuracy:\t0.85491+/-0.00003\n",
      "  valid auc-roc:\t0.91288+/-0.00002\n",
      "  valid auc-pr:\t\t0.90612+/-0.00803\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 3 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.36676 -- accuracy=85.54%  \n",
      "  valid loss:\t\t0.36356\n",
      "  valid accuracy:\t0.85665+/-0.00001\n",
      "  valid auc-roc:\t0.91432+/-0.00001\n",
      "  valid auc-pr:\t\t0.90777+/-0.00744\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 4 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.36314 -- accuracy=85.67%  \n",
      "  valid loss:\t\t0.36119\n",
      "  valid accuracy:\t0.85669+/-0.00000\n",
      "  valid auc-roc:\t0.91542+/-0.00000\n",
      "  valid auc-pr:\t\t0.90929+/-0.00713\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 5 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.36019 -- accuracy=85.80%  \n",
      "  valid loss:\t\t0.37366\n",
      "  valid accuracy:\t0.85379+/-0.00001\n",
      "  valid auc-roc:\t0.91603+/-0.00001\n",
      "  valid auc-pr:\t\t0.90999+/-0.00681\n",
      "Epoch 6 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35823 -- accuracy=85.86%  \n",
      "  valid loss:\t\t0.35703\n",
      "  valid accuracy:\t0.85875+/-0.00005\n",
      "  valid auc-roc:\t0.91754+/-0.00001\n",
      "  valid auc-pr:\t\t0.91175+/-0.00705\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 7 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35579 -- accuracy=85.95%  \n",
      "  valid loss:\t\t0.35795\n",
      "  valid accuracy:\t0.85855+/-0.00001\n",
      "  valid auc-roc:\t0.91776+/-0.00001\n",
      "  valid auc-pr:\t\t0.91206+/-0.00699\n",
      "Epoch 8 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35426 -- accuracy=86.02%  \n",
      "  valid loss:\t\t0.35726\n",
      "  valid accuracy:\t0.85856+/-0.00002\n",
      "  valid auc-roc:\t0.91834+/-0.00001\n",
      "  valid auc-pr:\t\t0.91271+/-0.00639\n",
      "Epoch 9 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35269 -- accuracy=86.05%  \n",
      "  valid loss:\t\t0.35408\n",
      "  valid accuracy:\t0.85972+/-0.00003\n",
      "  valid auc-roc:\t0.91877+/-0.00001\n",
      "  valid auc-pr:\t\t0.91318+/-0.00672\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 10 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35095 -- accuracy=86.14%  \n",
      "  valid loss:\t\t0.35467\n",
      "  valid accuracy:\t0.85988+/-0.00004\n",
      "  valid auc-roc:\t0.91924+/-0.00000\n",
      "  valid auc-pr:\t\t0.91362+/-0.00668\n",
      "Epoch 11 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34913 -- accuracy=86.18%  \n",
      "  valid loss:\t\t0.35595\n",
      "  valid accuracy:\t0.85936+/-0.00009\n",
      "  valid auc-roc:\t0.91916+/-0.00000\n",
      "  valid auc-pr:\t\t0.91365+/-0.00644\n",
      "Epoch 12 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34807 -- accuracy=86.22%  \n",
      "  valid loss:\t\t0.35404\n",
      "  valid accuracy:\t0.85987+/-0.00001\n",
      "  valid auc-roc:\t0.91950+/-0.00001\n",
      "  valid auc-pr:\t\t0.91386+/-0.00649\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 13 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34641 -- accuracy=86.27%  \n",
      "  valid loss:\t\t0.35325\n",
      "  valid accuracy:\t0.86060+/-0.00007\n",
      "  valid auc-roc:\t0.91935+/-0.00001\n",
      "  valid auc-pr:\t\t0.91368+/-0.00598\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 14 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34480 -- accuracy=86.34%  \n",
      "  valid loss:\t\t0.35227\n",
      "  valid accuracy:\t0.86052+/-0.00003\n",
      "  valid auc-roc:\t0.91996+/-0.00000\n",
      "  valid auc-pr:\t\t0.91447+/-0.00611\n",
      "saving model parameters to: ../results/test/original_model_best.pickle\n",
      "Epoch 15 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34411 -- accuracy=86.38%  \n",
      "  valid loss:\t\t0.35311\n",
      "  valid accuracy:\t0.86074+/-0.00007\n",
      "  valid auc-roc:\t0.91957+/-0.00000\n",
      "  valid auc-pr:\t\t0.91396+/-0.00608\n",
      "Epoch 16 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34252 -- accuracy=86.43%  \n",
      "  valid loss:\t\t0.35598\n",
      "  valid accuracy:\t0.85944+/-0.00006\n",
      "  valid auc-roc:\t0.91915+/-0.00000\n",
      "  valid auc-pr:\t\t0.91355+/-0.00627\n",
      "Epoch 17 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34159 -- accuracy=86.44%  \n",
      "  valid loss:\t\t0.35311\n",
      "  valid accuracy:\t0.86062+/-0.00006\n",
      "  valid auc-roc:\t0.91969+/-0.00001\n",
      "  valid auc-pr:\t\t0.91412+/-0.00607\n",
      "Epoch 18 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34026 -- accuracy=86.49%  \n",
      "  valid loss:\t\t0.35400\n",
      "  valid accuracy:\t0.86023+/-0.00002\n",
      "  valid auc-roc:\t0.91938+/-0.00001\n",
      "  valid auc-pr:\t\t0.91385+/-0.00591\n",
      "Epoch 19 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33941 -- accuracy=86.51%  \n",
      "  valid loss:\t\t0.36212\n",
      "  valid accuracy:\t0.85737+/-0.00003\n",
      "  valid auc-roc:\t0.91907+/-0.00000\n",
      "  valid auc-pr:\t\t0.91342+/-0.00601\n",
      "Epoch 20 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33811 -- accuracy=86.56%  \n",
      "  valid loss:\t\t0.35384\n",
      "  valid accuracy:\t0.86070+/-0.00001\n",
      "  valid auc-roc:\t0.91982+/-0.00001\n",
      "  valid auc-pr:\t\t0.91440+/-0.00585\n",
      "Epoch 21 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33710 -- accuracy=86.60%  \n",
      "  valid loss:\t\t0.35802\n",
      "  valid accuracy:\t0.85792+/-0.00002\n",
      "  valid auc-roc:\t0.91971+/-0.00000\n",
      "  valid auc-pr:\t\t0.91417+/-0.00561\n",
      "Epoch 22 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33591 -- accuracy=86.64%  \n",
      "  valid loss:\t\t0.35621\n",
      "  valid accuracy:\t0.85829+/-0.00004\n",
      "  valid auc-roc:\t0.91881+/-0.00002\n",
      "  valid auc-pr:\t\t0.91324+/-0.00591\n",
      "Epoch 23 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33509 -- accuracy=86.70%  \n",
      "  valid loss:\t\t0.35508\n",
      "  valid accuracy:\t0.85991+/-0.00001\n",
      "  valid auc-roc:\t0.91897+/-0.00000\n",
      "  valid auc-pr:\t\t0.91327+/-0.00600\n",
      "Epoch 24 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33395 -- accuracy=86.74%  \n",
      "  valid loss:\t\t0.35529\n",
      "  valid accuracy:\t0.86017+/-0.00001\n",
      "  valid auc-roc:\t0.91920+/-0.00001\n",
      "  valid auc-pr:\t\t0.91357+/-0.00586\n",
      "Epoch 25 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33365 -- accuracy=86.74%  \n",
      "  valid loss:\t\t0.35736\n",
      "  valid accuracy:\t0.85930+/-0.00003\n",
      "  valid auc-roc:\t0.91840+/-0.00002\n",
      "  valid auc-pr:\t\t0.91263+/-0.00590\n",
      "Patience ran out... Early stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<neuralnetwork.NeuralTrainer instance at 0x7f26ef154998>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test  loss:\t\t0.35232\n",
      "  test  accuracy:\t0.86048+/-0.00003\n",
      "  test  auc-roc:\t0.91995+/-0.00000\n",
      "  test  auc-pr:\t\t0.91446+/-0.00609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35231954771865853"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original model w/ batch norm and dropout and relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n"
     ]
    }
   ],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=18, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=40, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=15, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.NonlinearityLayer(net['conv3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=100, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'test')\n",
    "output_name = 'original_model_bells'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "savepath = utils.make_directory(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 500 \n",
      "[================              ] 54.0% -- time=27s -- loss=0.41420 -- accuracy=83.44%  "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# modified network (wider + batch norm + dropout + prelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=32, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.ParametricRectifierLayer(net['conv1_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=64, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.ParametricRectifierLayer(net['conv2_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=128, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.ParametricRectifierLayer(net['conv3_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=256, W=init.HeNormal(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "    net['dense1_norm'] = layers.BatchNormLayer(net['dense1'])\n",
    "    net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1_norm'], alpha=init.Constant(0.25))\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.5)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'test')\n",
    "output_name = 'original_model_bells_whistles'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "savepath = utils.make_directory(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=500, patience=10, verbose=1)\n",
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normal residual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "    def residual_block(net, last_layer, name, filter_size, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_filters = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.Conv2DLayer(net[last_layer], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.Conv2DLayer(net[name+'_1resid_dropout'], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=18, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv1_dropout1'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv1_dropout1', 'conv1_2', filter_size=(1,5), nonlinearity=nonlinearities.rectify)\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_2_resid'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=40, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv2_dropout1'] = layers.DropoutLayer(net['conv2_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv2_dropout1', 'conv2_2', filter_size=(1,5), nonlinearity=nonlinearities.rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_2_resid'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=15, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.NonlinearityLayer(net['conv3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=100, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'test')\n",
    "output_name = 'original_model_residual'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "savepath = utils.make_directory(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=500, patience=10, verbose=1)\n",
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# residual model (modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "    def residual_block(net, last_layer, name, filter_size, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_filters = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.Conv2DLayer(net[last_layer], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.Conv2DLayer(net[name+'_1resid_dropout'], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=50, filter_size=(3, 11), stride=(1, 1),    # 330\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv1_dropout1'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv1_dropout1', 'conv1_2', filter_size=(1,5), nonlinearity=nonlinearities.rectify)\n",
    "    net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_2_resid'], pool_size=(1, 10), stride=(1, 10)) # 32\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_pool'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=100, filter_size=(1, 6), stride=(1, 1), # 27\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv2_dropout1'] = layers.DropoutLayer(net['conv2_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv2_dropout1', 'conv2_2', filter_size=(1,5), nonlinearity=nonlinearities.rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_2_resid'], pool_size=(1, 9), stride=(1, 9)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv2_dropout'], num_units=256, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.0003, \n",
    "                    \"l2\": 1e-5\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'test')\n",
    "output_name = '1D_model_resid'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "savepath = utils.make_directory(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: ../results/test/1d_version\n",
      "compiling model\n"
     ]
    }
   ],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=32, filter_size=(3, 6), stride=(1, 1),    # 330\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.ParametricRectifierLayer(net['conv1_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_active'], pool_size=(1, 5), stride=(1, 5)) # 65\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_pool'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=64, filter_size=(1, 6), stride=(1, 1), # 60\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.ParametricRectifierLayer(net['conv2_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(1, 5), stride=(1, 5)) # 12\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=128, filter_size=(1, 4), stride=(1, 1), # 8\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.ParametricRectifierLayer(net['conv3_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv3_pool'] = layers.MaxPool2DLayer(net['conv3_active'], pool_size=(1, 3), stride=(1, 3)) # 25\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_pool'], p=0.1)\n",
    "    \n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=256, W=init.HeNormal(), \n",
    "                                     b=init.Constant(0.1), nonlinearity=None)\n",
    "    net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1'], alpha=init.Constant(0.25))\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.0003, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'test')\n",
    "output_name = '1d_version'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "savepath = utils.make_directory(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.40147 -- accuracy=84.37%  \n",
      "  valid loss:\t\t0.37892\n",
      "  valid accuracy:\t0.85196+/-0.00007\n",
      "  valid auc-roc:\t0.90785+/-0.00000\n",
      "  valid auc-pr:\t\t0.89887+/-0.00923\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 2 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.37666 -- accuracy=85.37%  \n",
      "  valid loss:\t\t0.36710\n",
      "  valid accuracy:\t0.85686+/-0.00005\n",
      "  valid auc-roc:\t0.91342+/-0.00000\n",
      "  valid auc-pr:\t\t0.90614+/-0.00795\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 3 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.36937 -- accuracy=85.60%  \n",
      "  valid loss:\t\t0.36464\n",
      "  valid accuracy:\t0.85817+/-0.00000\n",
      "  valid auc-roc:\t0.91527+/-0.00000\n",
      "  valid auc-pr:\t\t0.90877+/-0.00711\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 4 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.36458 -- accuracy=85.75%  \n",
      "  valid loss:\t\t0.36341\n",
      "  valid accuracy:\t0.85742+/-0.00001\n",
      "  valid auc-roc:\t0.91765+/-0.00001\n",
      "  valid auc-pr:\t\t0.91151+/-0.00628\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 5 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.36163 -- accuracy=85.83%  \n",
      "  valid loss:\t\t0.35687\n",
      "  valid accuracy:\t0.85922+/-0.00000\n",
      "  valid auc-roc:\t0.91838+/-0.00000\n",
      "  valid auc-pr:\t\t0.91251+/-0.00589\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 6 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35893 -- accuracy=85.93%  \n",
      "  valid loss:\t\t0.35546\n",
      "  valid accuracy:\t0.86006+/-0.00001\n",
      "  valid auc-roc:\t0.91959+/-0.00000\n",
      "  valid auc-pr:\t\t0.91405+/-0.00564\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 7 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35696 -- accuracy=85.99%  \n",
      "  valid loss:\t\t0.36301\n",
      "  valid accuracy:\t0.85859+/-0.00002\n",
      "  valid auc-roc:\t0.92057+/-0.00001\n",
      "  valid auc-pr:\t\t0.91511+/-0.00527\n",
      "Epoch 8 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35520 -- accuracy=86.05%  \n",
      "  valid loss:\t\t0.35666\n",
      "  valid accuracy:\t0.86029+/-0.00001\n",
      "  valid auc-roc:\t0.92077+/-0.00000\n",
      "  valid auc-pr:\t\t0.91538+/-0.00502\n",
      "Epoch 9 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35375 -- accuracy=86.09%  \n",
      "  valid loss:\t\t0.35390\n",
      "  valid accuracy:\t0.86105+/-0.00001\n",
      "  valid auc-roc:\t0.92170+/-0.00000\n",
      "  valid auc-pr:\t\t0.91649+/-0.00463\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 10 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35252 -- accuracy=86.12%  \n",
      "  valid loss:\t\t0.35410\n",
      "  valid accuracy:\t0.86008+/-0.00000\n",
      "  valid auc-roc:\t0.92131+/-0.00000\n",
      "  valid auc-pr:\t\t0.91604+/-0.00473\n",
      "Epoch 11 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35127 -- accuracy=86.18%  \n",
      "  valid loss:\t\t0.34887\n",
      "  valid accuracy:\t0.86173+/-0.00000\n",
      "  valid auc-roc:\t0.92220+/-0.00000\n",
      "  valid auc-pr:\t\t0.91700+/-0.00447\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 12 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.35028 -- accuracy=86.17%  \n",
      "  valid loss:\t\t0.34939\n",
      "  valid accuracy:\t0.86149+/-0.00000\n",
      "  valid auc-roc:\t0.92251+/-0.00000\n",
      "  valid auc-pr:\t\t0.91746+/-0.00435\n",
      "Epoch 13 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34886 -- accuracy=86.22%  \n",
      "  valid loss:\t\t0.35036\n",
      "  valid accuracy:\t0.86155+/-0.00000\n",
      "  valid auc-roc:\t0.92242+/-0.00000\n",
      "  valid auc-pr:\t\t0.91731+/-0.00425\n",
      "Epoch 14 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34825 -- accuracy=86.25%  \n",
      "  valid loss:\t\t0.34827\n",
      "  valid accuracy:\t0.86216+/-0.00000\n",
      "  valid auc-roc:\t0.92282+/-0.00000\n",
      "  valid auc-pr:\t\t0.91789+/-0.00424\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 15 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34730 -- accuracy=86.28%  \n",
      "  valid loss:\t\t0.34839\n",
      "  valid accuracy:\t0.86235+/-0.00000\n",
      "  valid auc-roc:\t0.92283+/-0.00000\n",
      "  valid auc-pr:\t\t0.91778+/-0.00418\n",
      "Epoch 16 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34648 -- accuracy=86.30%  \n",
      "  valid loss:\t\t0.34650\n",
      "  valid accuracy:\t0.86253+/-0.00000\n",
      "  valid auc-roc:\t0.92320+/-0.00000\n",
      "  valid auc-pr:\t\t0.91835+/-0.00439\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 17 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34548 -- accuracy=86.34%  \n",
      "  valid loss:\t\t0.34578\n",
      "  valid accuracy:\t0.86218+/-0.00000\n",
      "  valid auc-roc:\t0.92361+/-0.00000\n",
      "  valid auc-pr:\t\t0.91885+/-0.00424\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 18 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34462 -- accuracy=86.37%  \n",
      "  valid loss:\t\t0.34784\n",
      "  valid accuracy:\t0.86278+/-0.00000\n",
      "  valid auc-roc:\t0.92359+/-0.00000\n",
      "  valid auc-pr:\t\t0.91880+/-0.00428\n",
      "Epoch 19 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34411 -- accuracy=86.38%  \n",
      "  valid loss:\t\t0.34801\n",
      "  valid accuracy:\t0.86191+/-0.00000\n",
      "  valid auc-roc:\t0.92397+/-0.00000\n",
      "  valid auc-pr:\t\t0.91913+/-0.00401\n",
      "Epoch 20 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34322 -- accuracy=86.43%  \n",
      "  valid loss:\t\t0.34837\n",
      "  valid accuracy:\t0.86271+/-0.00000\n",
      "  valid auc-roc:\t0.92371+/-0.00000\n",
      "  valid auc-pr:\t\t0.91899+/-0.00427\n",
      "Epoch 21 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34271 -- accuracy=86.45%  \n",
      "  valid loss:\t\t0.34527\n",
      "  valid accuracy:\t0.86319+/-0.00000\n",
      "  valid auc-roc:\t0.92421+/-0.00000\n",
      "  valid auc-pr:\t\t0.91952+/-0.00424\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 22 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34198 -- accuracy=86.47%  \n",
      "  valid loss:\t\t0.34798\n",
      "  valid accuracy:\t0.86244+/-0.00000\n",
      "  valid auc-roc:\t0.92374+/-0.00000\n",
      "  valid auc-pr:\t\t0.91898+/-0.00401\n",
      "Epoch 23 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34153 -- accuracy=86.48%  \n",
      "  valid loss:\t\t0.34979\n",
      "  valid accuracy:\t0.86121+/-0.00000\n",
      "  valid auc-roc:\t0.92348+/-0.00000\n",
      "  valid auc-pr:\t\t0.91871+/-0.00378\n",
      "Epoch 24 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34119 -- accuracy=86.47%  \n",
      "  valid loss:\t\t0.34948\n",
      "  valid accuracy:\t0.86161+/-0.00000\n",
      "  valid auc-roc:\t0.92400+/-0.00000\n",
      "  valid auc-pr:\t\t0.91928+/-0.00365\n",
      "Epoch 25 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34052 -- accuracy=86.54%  \n",
      "  valid loss:\t\t0.34645\n",
      "  valid accuracy:\t0.86275+/-0.00000\n",
      "  valid auc-roc:\t0.92393+/-0.00000\n",
      "  valid auc-pr:\t\t0.91917+/-0.00401\n",
      "Epoch 26 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34024 -- accuracy=86.50%  \n",
      "  valid loss:\t\t0.34615\n",
      "  valid accuracy:\t0.86285+/-0.00000\n",
      "  valid auc-roc:\t0.92401+/-0.00000\n",
      "  valid auc-pr:\t\t0.91924+/-0.00426\n",
      "Epoch 27 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34013 -- accuracy=86.50%  \n",
      "  valid loss:\t\t0.34465\n",
      "  valid accuracy:\t0.86312+/-0.00000\n",
      "  valid auc-roc:\t0.92459+/-0.00000\n",
      "  valid auc-pr:\t\t0.91986+/-0.00385\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 28 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33935 -- accuracy=86.55%  \n",
      "  valid loss:\t\t0.34618\n",
      "  valid accuracy:\t0.86261+/-0.00000\n",
      "  valid auc-roc:\t0.92429+/-0.00000\n",
      "  valid auc-pr:\t\t0.91959+/-0.00391\n",
      "Epoch 29 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33955 -- accuracy=86.54%  \n",
      "  valid loss:\t\t0.34418\n",
      "  valid accuracy:\t0.86303+/-0.00000\n",
      "  valid auc-roc:\t0.92433+/-0.00000\n",
      "  valid auc-pr:\t\t0.91963+/-0.00392\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 30 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33875 -- accuracy=86.59%  \n",
      "  valid loss:\t\t0.34634\n",
      "  valid accuracy:\t0.86216+/-0.00000\n",
      "  valid auc-roc:\t0.92449+/-0.00000\n",
      "  valid auc-pr:\t\t0.91992+/-0.00377\n",
      "Epoch 31 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33865 -- accuracy=86.58%  \n",
      "  valid loss:\t\t0.34710\n",
      "  valid accuracy:\t0.86171+/-0.00000\n",
      "  valid auc-roc:\t0.92434+/-0.00000\n",
      "  valid auc-pr:\t\t0.91953+/-0.00367\n",
      "Epoch 32 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33814 -- accuracy=86.58%  \n",
      "  valid loss:\t\t0.34650\n",
      "  valid accuracy:\t0.86290+/-0.00000\n",
      "  valid auc-roc:\t0.92431+/-0.00000\n",
      "  valid auc-pr:\t\t0.91957+/-0.00354\n",
      "Epoch 33 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33804 -- accuracy=86.61%  \n",
      "  valid loss:\t\t0.34389\n",
      "  valid accuracy:\t0.86331+/-0.00000\n",
      "  valid auc-roc:\t0.92477+/-0.00000\n",
      "  valid auc-pr:\t\t0.92022+/-0.00369\n",
      "saving model parameters to: ../results/test/1d_version_best.pickle\n",
      "Epoch 34 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33762 -- accuracy=86.58%  \n",
      "  valid loss:\t\t0.34665\n",
      "  valid accuracy:\t0.86238+/-0.00000\n",
      "  valid auc-roc:\t0.92434+/-0.00000\n",
      "  valid auc-pr:\t\t0.91967+/-0.00378\n",
      "Epoch 35 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33750 -- accuracy=86.61%  \n",
      "  valid loss:\t\t0.34470\n",
      "  valid accuracy:\t0.86293+/-0.00000\n",
      "  valid auc-roc:\t0.92448+/-0.00000\n",
      "  valid auc-pr:\t\t0.91977+/-0.00365\n",
      "Epoch 36 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33709 -- accuracy=86.61%  \n",
      "  valid loss:\t\t0.34498\n",
      "  valid accuracy:\t0.86288+/-0.00000\n",
      "  valid auc-roc:\t0.92441+/-0.00000\n",
      "  valid auc-pr:\t\t0.91973+/-0.00385\n",
      "Epoch 37 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33701 -- accuracy=86.61%  \n",
      "  valid loss:\t\t0.34600\n",
      "  valid accuracy:\t0.86273+/-0.00000\n",
      "  valid auc-roc:\t0.92416+/-0.00000\n",
      "  valid auc-pr:\t\t0.91949+/-0.00402\n",
      "Epoch 38 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33676 -- accuracy=86.63%  \n",
      "  valid loss:\t\t0.34775\n",
      "  valid accuracy:\t0.86229+/-0.00000\n",
      "  valid auc-roc:\t0.92397+/-0.00000\n",
      "  valid auc-pr:\t\t0.91921+/-0.00359\n",
      "Epoch 39 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33648 -- accuracy=86.63%  \n",
      "  valid loss:\t\t0.34771\n",
      "  valid accuracy:\t0.86290+/-0.00000\n",
      "  valid auc-roc:\t0.92430+/-0.00000\n",
      "  valid auc-pr:\t\t0.91962+/-0.00390\n",
      "Epoch 40 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33610 -- accuracy=86.66%  \n",
      "  valid loss:\t\t0.34627\n",
      "  valid accuracy:\t0.86248+/-0.00000\n",
      "  valid auc-roc:\t0.92450+/-0.00000\n",
      "  valid auc-pr:\t\t0.91978+/-0.00365\n",
      "Epoch 41 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33617 -- accuracy=86.66%  \n",
      "  valid loss:\t\t0.34673\n",
      "  valid accuracy:\t0.86324+/-0.00000\n",
      "  valid auc-roc:\t0.92417+/-0.00000\n",
      "  valid auc-pr:\t\t0.91954+/-0.00372\n",
      "Epoch 42 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33578 -- accuracy=86.67%  \n",
      "  valid loss:\t\t0.34760\n",
      "  valid accuracy:\t0.86280+/-0.00000\n",
      "  valid auc-roc:\t0.92406+/-0.00000\n",
      "  valid auc-pr:\t\t0.91939+/-0.00355\n",
      "Epoch 43 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33594 -- accuracy=86.66%  \n",
      "  valid loss:\t\t0.34660\n",
      "  valid accuracy:\t0.86272+/-0.00000\n",
      "  valid auc-roc:\t0.92434+/-0.00000\n",
      "  valid auc-pr:\t\t0.91961+/-0.00355\n",
      "Epoch 44 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.33551 -- accuracy=86.66%  \n",
      "  valid loss:\t\t0.34761\n",
      "  valid accuracy:\t0.86231+/-0.00000\n",
      "  valid auc-roc:\t0.92405+/-0.00000\n",
      "  valid auc-pr:\t\t0.91915+/-0.00338\n",
      "Patience ran out... Early stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<neuralnetwork.NeuralTrainer instance at 0x7f4696804b48>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test  loss:\t\t0.34389\n",
      "  test  accuracy:\t0.86331+/-0.00000\n",
      "  test  auc-roc:\t0.92477+/-0.00000\n",
      "  test  auc-pr:\t\t0.92022+/-0.00369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34388726719256413"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
