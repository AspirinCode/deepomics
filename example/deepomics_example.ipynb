{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 1: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/home/peter/Code/deepomics')\n",
    "from neuralnetwork import NeuralNet, NeuralTrainer\n",
    "import train as fit \n",
    "import visualize, utils\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 476 ms, sys: 1.11 s, total: 1.59 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "data_path = '/home/peter/Code/tensorflow/data'\n",
    "file_path = os.path.join(data_path, filename)\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(file_path,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "train = (dtf.transpose([0,3,1,2]), ltf)\n",
    "valid = (dtf_crossval.transpose([0,3,1,2]), ltf_crossval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validation_results(nnmodel):\n",
    "    def load_validation_data(file_path):\n",
    "        group_name = ['valid_data']\n",
    "        dataset = h5py.File(file_path,'r')\n",
    "        val_dat = np.array(dataset['/'+group_name[0]+'/vs_valid'])\n",
    "        val_lbl = np.array(dataset['/'+group_name[0]+'/label_valid'])\n",
    "        return val_dat, val_lbl\n",
    "\n",
    "    # load data\n",
    "    file_path = os.path.join(data_path,'valideval_dataset.hdf5')\n",
    "    val_dat, val_lbl = load_validation_data(file_path)\n",
    "\n",
    "    N=140\n",
    "    fragLen=330\n",
    "\n",
    "    avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "    startgap = np.ceil((val_dat.shape[1] - fragLen)/14.).astype('int')\n",
    "    true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "\n",
    "    # Counter for the \"true_lbl\" array\n",
    "    cnt_ = 0\n",
    "    # Counter for the \"pred_lbl\" array\n",
    "    cnt_u = 0\n",
    "\n",
    "    for a in range(val_dat.shape[0]):\n",
    "        if a%100 == 0:\n",
    "            print(a)\n",
    "\n",
    "        # Create batch array to send thru network\n",
    "        im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "\n",
    "        # Count the number of traces in each batch\n",
    "        cnt = 0\n",
    "\n",
    "        for b in range(val_dat.shape[0]):\n",
    "\n",
    "            for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "                try:\n",
    "                    im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                         val_dat[b,n:n+fragLen],\n",
    "                                         avg_F[n:n+fragLen]))\n",
    "                except:\n",
    "                    from IPython.core.debugger import Tracer\n",
    "                    Tracer()()\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "            # Keep track of the true labels\n",
    "            if val_lbl[a,b] == 1:\n",
    "                true_lbl[cnt_] = 1\n",
    "            else:\n",
    "                true_lbl[cnt_] = 0\n",
    "\n",
    "            cnt_ += 1\n",
    "\n",
    "        # Run batch through network\n",
    "        pred_stop = nnmodel.get_feature_maps(layer='output', X=im_eval.transpose([0,3,1,2]))\n",
    "\n",
    "        # Average output over each group of N traces\n",
    "        for u in range(0, len(pred_stop), N):\n",
    "            pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N,0])\n",
    "            cnt_u += 1       \n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "    return auc(fpr, tpr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=18, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=init.Constant(0.1), \n",
    "                                      nonlinearity=nonlinearities.tanh, pad='valid')\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1'], num_filters=40, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=init.Constant(0.1), \n",
    "                                      nonlinearity=nonlinearities.tanh, pad='valid')\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_pool'], num_filters=15, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=init.Constant(0.1), \n",
    "                                      nonlinearity=nonlinearities.tanh, pad='valid')\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3'], num_units=100, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=nonlinearities.rectify)\n",
    "    \n",
    "    net['output'] = layers.DenseLayer(net['dense1'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = 'original_model'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check official validation error\n",
    "auc = validation_results(nnmodel)\n",
    "print('validation AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# original model w/ batch norm and dropout and relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=18, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=40, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=15, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.NonlinearityLayer(net['conv3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=100, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = 'original_model_bells'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=500, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check official validation error\n",
    "auc = validation_results(nnmodel)\n",
    "print('validation AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# modified network (wider + batch norm + dropout + prelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=32, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.ParametricRectifierLayer(net['conv1_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=64, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.ParametricRectifierLayer(net['conv2_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=128, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.ParametricRectifierLayer(net['conv3_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=256, W=init.HeNormal(), \n",
    "                                     b=init.Constant(0.1), nonlinearity=None)\n",
    "    net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1'], alpha=init.Constant(0.25))\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_active'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = 'original_model_bells_whistles'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=20, patience=5, verbose=1)\n",
    "# train model\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=500, num_epochs=20, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=10, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1500, num_epochs=10, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2000, num_epochs=10, patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check official validation error\n",
    "auc = validation_results(nnmodel)\n",
    "print('validation AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# normal residual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "    def residual_block(net, last_layer, name, filter_size, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_filters = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.Conv2DLayer(net[last_layer], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.Conv2DLayer(net[name+'_1resid_dropout'], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "    def residual_block2(net, last_layer, name, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_units = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.DenseLayer(net[last_layer], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.DenseLayer(net[name+'_1resid_dropout'], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=18, filter_size=(2, 5), stride=(1, 1),    # 1000\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_dropout1'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv1_dropout1', 'conv1_2', filter_size=(3,5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_2_resid'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=40, filter_size=(2, 5), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv2_dropout1'] = layers.DropoutLayer(net['conv2_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv2_dropout1', 'conv2_2', filter_size=(1,5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_2_resid'], pool_size=(1, 10), stride=(1, 10)) # 25\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=15, filter_size=(1, 1), stride=(1, 1), # 18\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.NonlinearityLayer(net['conv3_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=100, W=init.HeNormal(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "    net['dense1_norm'] = layers.BatchNormLayer(net['dense1'])    \n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout1'] = layers.DropoutLayer(net['dense1_active'], p=0.1)\n",
    "    net = residual_block2(net, 'dense1_dropout1', 'dense1_2', nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_2_resid'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = 'original_model_residual'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=20, patience=5, verbose=1)\n",
    "# train model\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=500, num_epochs=20, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=10, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1500, num_epochs=10, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2000, num_epochs=10, patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check official validation error\n",
    "auc = validation_results(nnmodel)\n",
    "print('validation AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# residual model (modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda2/lib/python2.7/site-packages/lasagne/layers/pool.py:267: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode=self.mode,\n",
      "/home/peter/anaconda2/lib/python2.7/site-packages/lasagne/layers/pool.py:267: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode=self.mode,\n",
      "/home/peter/anaconda2/lib/python2.7/site-packages/lasagne/layers/pool.py:267: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode=self.mode,\n"
     ]
    }
   ],
   "source": [
    "def build_model(shape, num_labels):\n",
    "    def residual_block(net, last_layer, name, filter_size, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_filters = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.Conv2DLayer(net[last_layer], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.ParametricRectifierLayer(net[name+'_1resid_norm'], alpha=init.Constant(0.25))\n",
    "            \n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.Conv2DLayer(net[name+'_1resid_dropout'], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.ParametricRectifierLayer(net[name+'_residual'], alpha=init.Constant(0.25))\n",
    "\n",
    "        return net\n",
    "    \n",
    "    def residual_block2(net, last_layer, name, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_units = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.DenseLayer(net[last_layer], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.ParametricRectifierLayer(net[name+'_1resid_norm'], alpha=init.Constant(0.25))\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.2)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.DenseLayer(net[name+'_1resid_dropout'], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.ParametricRectifierLayer(net[name+'_residual'], alpha=init.Constant(0.25))\n",
    "\n",
    "        return net\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=32, filter_size=(2, 5), stride=(1, 1),    # 325\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.ParametricRectifierLayer(net['conv1_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv1_dropout1'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv1_dropout1', 'conv1_2', filter_size=(3, 5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_2_resid'], p=0.2)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=64, filter_size=(2, 5), stride=(1, 1), # 60\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.ParametricRectifierLayer(net['conv2_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv2_dropout1'] = layers.DropoutLayer(net['conv2_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv2_dropout1', 'conv2_2', filter_size=(1, 5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_2_resid'], pool_size=(1, 10), stride=(1, 10)) # 12\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.2)\n",
    "\n",
    "    net['conv3'] = layers.Conv2DLayer(net['conv2_dropout'], num_filters=128, filter_size=(1, 1), stride=(1, 1), # 9\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "    net['conv3_active'] = layers.ParametricRectifierLayer(net['conv3_norm'], alpha=init.Constant(0.25))\n",
    "    net['conv3_dropout'] = layers.DropoutLayer(net['conv3_active'], p=0.2)\n",
    "    \n",
    "    net['dense1'] = layers.DenseLayer(net['conv3_dropout'], num_units=256, W=init.HeNormal(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "    net['dense1_norm'] = layers.BatchNormLayer(net['dense1'])    \n",
    "    net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1_norm'], alpha=init.Constant(0.25))\n",
    "    net['dense1_dropout1'] = layers.DropoutLayer(net['dense1_active'], p=0.2)\n",
    "    net = residual_block2(net, 'dense1_dropout1', 'dense1_2', nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_2_resid'], p=0.5)\n",
    "        \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "        \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = '1D_model_resid'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 50 \n",
      "[===                           ] 11.4% -- time=519s -- loss=0.42606 -- accuracy=83.90%  "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "#nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=50, patience=10, verbose=1)\n",
    "# train model\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=500, num_epochs=50, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1500, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2000, num_epochs=50, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=50, patience=10, verbose=1)\n",
    "# train model\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=500, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1500, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2000, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2500, num_epochs=50, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def validation_score(nntrainer, data_path):\n",
    "\n",
    "    filename = 'valideval_dataset.hdf5'\n",
    "    dataset = h5py.File(os.path.join(data_path,filename),'r')\n",
    "    group_name = ['valid_data']\n",
    "    val_dat = np.array(dataset['/'+group_name[0]+'/vs_valid'])\n",
    "    val_lbl = np.array(dataset['/'+group_name[0]+'/label_valid'])\n",
    "\n",
    "    fragLen = 330\n",
    "    N = 14\n",
    "    avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "    startgap = np.ceil(float(val_dat.shape[1] - fragLen)/N).astype('int')\n",
    "    true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "\n",
    "    # Counter for the \"true_lbl\" array\n",
    "    cnt_ = 0\n",
    "    # Counter for the \"pred_lbl\" array\n",
    "    cnt_u = 0\n",
    "    for a in range(val_dat.shape[0]):\n",
    "        if a%100 == 0:\n",
    "            print('\\r' + 'X'*(a//100))\n",
    "\n",
    "        # Create batch array to send thru network\n",
    "        im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "\n",
    "        # Count the number of traces in each batch\n",
    "        cnt = 0\n",
    "\n",
    "        for b in range(val_dat.shape[0]):\n",
    "\n",
    "            for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "                try:\n",
    "                    im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                         val_dat[b,n:n+fragLen],\n",
    "                                         avg_F[n:n+fragLen]))\n",
    "                except:\n",
    "                    from IPython.core.debugger import Tracer\n",
    "                    Tracer()()\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "            # Keep track of the true labels\n",
    "            if val_lbl[a,b] == 1:\n",
    "                true_lbl[cnt_] = 1\n",
    "            else:\n",
    "                true_lbl[cnt_] = 0\n",
    "\n",
    "            cnt_ += 1\n",
    "\n",
    "        # Run batch through network\n",
    "        pred_stop = nntrainer.nnmodel.get_feature_maps(layer='output', X=im_eval.transpose([0,3,1,2]))[:,0]\n",
    "        # Average output over each group of N traces\n",
    "        for u in range(0, len(pred_stop), N):\n",
    "            pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N])\n",
    "            cnt_u += 1        \n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "    fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "    wrk = auc(fpr, tpr)\n",
    "    print(wrk)\n",
    "    return wrk\n",
    "\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1D models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1D architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "    def residual_block(net, last_layer, name, filter_size, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_filters = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.Conv2DLayer(net[last_layer], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.Conv2DLayer(net[name+'_1resid_dropout'], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "    def residual_block2(net, last_layer, name, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_units = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.DenseLayer(net[last_layer], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.DenseLayer(net[name+'_1resid_dropout'], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "    \n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=32, filter_size=(3, 11), stride=(1, 1),    # 320\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_dropout1'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv1_dropout1', 'conv1_2', filter_size=(1,5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_2_resid'], pool_size=(1, 32), stride=(1, 32)) # 13\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_pool'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv1_dropout'], num_units=128, W=init.HeNormal(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "    net['dense1_norm'] = layers.BatchNormLayer(net['dense1'])    \n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout1'] = layers.DropoutLayer(net['dense1_active'], p=0.1)\n",
    "    net = residual_block2(net, 'dense1_dropout1', 'dense1_2', nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_2_resid'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = '1d_version'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=20, patience=5, verbose=1)\n",
    "# train model\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=500, num_epochs=20, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=10, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1500, num_epochs=10, patience=5, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2000, num_epochs=10, patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check official validation error\n",
    "auc = validation_results(nnmodel)\n",
    "print('validation AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1D residual model (modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, num_labels):\n",
    "    def residual_block(net, last_layer, name, filter_size, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_filters = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.Conv2DLayer(net[last_layer], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.Conv2DLayer(net[name+'_1resid_dropout'], num_filters=num_filters, filter_size=filter_size, stride=(1, 1),    # 1000\n",
    "                         W=init.HeNormal(), b=None, nonlinearity=None, pad='same')\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "    \n",
    "    def residual_block2(net, last_layer, name, nonlinearity=nonlinearities.rectify):\n",
    "        # original residual unit\n",
    "        shape = layers.get_output_shape(net[last_layer])\n",
    "        num_units = shape[1]\n",
    "\n",
    "        net[name+'_1resid'] = layers.DenseLayer(net[last_layer], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_1resid_norm'] = layers.BatchNormLayer(net[name+'_1resid'])\n",
    "        net[name+'_1resid_active'] = layers.NonlinearityLayer(net[name+'_1resid_norm'], nonlinearity=nonlinearity)\n",
    "\n",
    "        net[name+'_1resid_dropout'] = layers.DropoutLayer(net[name+'_1resid_active'], p=0.1)\n",
    "\n",
    "        # bottleneck residual layer\n",
    "        net[name+'_2resid'] = layers.DenseLayer(net[name+'_1resid_dropout'], num_units=num_units, W=init.HeNormal(), b=None, nonlinearity=None)\n",
    "        net[name+'_2resid_norm'] = layers.BatchNormLayer(net[name+'_2resid'])\n",
    "\n",
    "        # combine input with residuals\n",
    "        net[name+'_residual'] = layers.ElemwiseSumLayer([net[last_layer], net[name+'_2resid_norm']])\n",
    "        net[name+'_resid'] = layers.NonlinearityLayer(net[name+'_residual'], nonlinearity=nonlinearity)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    # get model\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.dmatrix('targets')\n",
    "\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(input_var=input_var, shape=shape) # 330\n",
    "    net['conv1'] = layers.Conv2DLayer(net['input'], num_filters=32, filter_size=(3, 7), stride=(1, 1), # 324\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "    net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_dropout1'] = layers.DropoutLayer(net['conv1_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv1_dropout1', 'conv1_2', filter_size=(1,5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_2_resid'], pool_size=(1, 12), stride=(1, 12)) # 27\n",
    "    net['conv1_dropout'] = layers.DropoutLayer(net['conv1_pool'], p=0.1)\n",
    "\n",
    "    net['conv2'] = layers.Conv2DLayer(net['conv1_dropout'], num_filters=64, filter_size=(1, 7), stride=(1, 1), # 21\n",
    "                                        W=init.HeNormal(), b=None, nonlinearity=None, pad='valid')\n",
    "    net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "    net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv2_dropout1'] = layers.DropoutLayer(net['conv2_active'], p=0.1)\n",
    "    net = residual_block(net, 'conv2_dropout1', 'conv2_2', filter_size=(1,5), nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_2_resid'], pool_size=(1, 7), stride=(1, 7)) # 3\n",
    "    net['conv2_dropout'] = layers.DropoutLayer(net['conv2_pool'], p=0.1)\n",
    "\n",
    "    net['dense1'] = layers.DenseLayer(net['conv2_dropout'], num_units=128, W=init.HeNormal(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "    net['dense1_norm'] = layers.BatchNormLayer(net['dense1'])    \n",
    "    net['dense1_active'] = layers.NonlinearityLayer(net['dense1_norm'], nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout1'] = layers.DropoutLayer(net['dense1_active'], p=0.1)\n",
    "    net = residual_block2(net, 'dense1_dropout1', 'dense1_2', nonlinearity=nonlinearities.leaky_rectify)\n",
    "    net['dense1_dropout'] = layers.DropoutLayer(net['dense1_2_resid'], p=0.3)\n",
    "    \n",
    "    net['dense2'] = layers.DenseLayer(net['dense1_dropout'], num_units=num_labels, W=init.HeNormal(), \n",
    "                                     b=init.Constant(), nonlinearity=None)\n",
    "    net['output'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.sigmoid)\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001, \n",
    "                    \"l2\": 1e-6\n",
    "                    }\n",
    "\n",
    "    return net, input_var, target_var, optimization\n",
    "\n",
    "# build network\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n",
    "net, input_var, target_var, optimization = build_model(shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "#nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory(data_path, 'results')\n",
    "resultspath = utils.make_directory(resultspath, 'deepomics')\n",
    "output_name = '1D_model_resid'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "#nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=100, num_epochs=50, patience=10, verbose=1)\n",
    "# train model\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=500, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1000, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=1500, num_epochs=50, patience=10, verbose=1)\n",
    "\n",
    "nnmodel = NeuralNet(net, input_var, target_var)\n",
    "nntrainer = NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "nntrainer.set_best_parameters()\n",
    "fit.train_minibatch(nntrainer, data={'train': train, 'valid': valid}, \n",
    "                              batch_size=2000, num_epochs=50, patience=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "\n",
    "# test model\n",
    "nntrainer.test_model(valid, batch_size=100, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check official validation error\n",
    "auc = validation_results(nnmodel)\n",
    "print('validation AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
