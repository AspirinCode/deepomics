{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics import utils, fit, init, visualize, saliency\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_simulation(filepath):\n",
    "    # setup paths for file handling\n",
    "\n",
    "    trainmat = h5py.File(filepath, 'r')\n",
    "\n",
    "    print(\"loading training data\")\n",
    "    X_train = np.array(trainmat['X_train']).astype(np.float32)\n",
    "    y_train = np.array(trainmat['Y_train']).astype(np.float32)\n",
    "\n",
    "    print(\"loading cross-validation data\")\n",
    "    X_valid = np.array(trainmat['X_valid']).astype(np.float32)\n",
    "    y_valid = np.array(trainmat['Y_valid']).astype(np.int32)\n",
    "\n",
    "    print(\"loading test data\")\n",
    "    X_test = np.array(trainmat['X_test']).astype(np.float32)\n",
    "    y_test = np.array(trainmat['Y_test']).astype(np.int32)\n",
    "\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=3).transpose([0,2,3,1])\n",
    "    X_valid = np.expand_dims(X_valid, axis=3).transpose([0,2,3,1])\n",
    "    X_test = np.expand_dims(X_test, axis=3).transpose([0,2,3,1])\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "\n",
    "# load data\n",
    "filename = 'Synthetic_TF_dataset_50000.h5'\n",
    "data_path = '../../genomic_representations/data/synthetic_TF_dataset'\n",
    "file_path = os.path.join(data_path, filename)\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_simulation(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def model(input_shape, output_shape):\n",
    "\n",
    "    # create model\n",
    "    layer1 = {'layer': 'input',\n",
    "            'input_shape': input_shape\n",
    "            }\n",
    "    layer2 = {'layer': 'conv1d', \n",
    "            'num_filters': 32,\n",
    "            'filter_size': 11,\n",
    "            'norm': 'batch',\n",
    "            'activation': 'relu',\n",
    "            'dropout': 0.1,\n",
    "            'padding': 'SAME',\n",
    "            'pool_size': 50,\n",
    "            }\n",
    "    layer3 = {'layer': 'dense', \n",
    "            'num_units': 48,\n",
    "            'norm': 'batch',\n",
    "            'activation': 'relu',\n",
    "            'dropout': 0.5\n",
    "            }  \n",
    "    layer4 = {'layer': 'dense', \n",
    "            'num_units': output_shape[1],\n",
    "            'activation': 'sigmoid'\n",
    "            }\n",
    "\n",
    "    #from tfomics import build_network\n",
    "    model_layers = [layer1, layer2, layer3, layer4]\n",
    "    \n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                  \"optimizer\": \"adam\",\n",
    "                  \"learning_rate\": 0.001, \n",
    "                  \"l2\": 1e-6,\n",
    "                  #\"l1\": 0.1, \n",
    "                  }\n",
    "    return model_layers, optimization\n",
    "\n",
    "\n",
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "output_shape = y_train.shape\n",
    "\n",
    "model_layers, optimization = model(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'tmp')\n",
    "output_name = 'test'\n",
    "file_path = os.path.join(results_path, output_name)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet()\n",
    "nnmodel.build_layers(model_layers, optimization)\n",
    "# nnmodel.inspect_layers()\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = utils.initialize_session(nnmodel.placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid}\n",
    "test = {'inputs': X_test, 'targets': y_test}\n",
    "data = {'train': train, 'valid': valid, 'test': test}\n",
    "fit.train_minibatch(sess, nntrainer, data, batch_size=200, num_epochs=50, \n",
    "                      patience=20, verbose=2, shuffle=True, save_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters(sess)\n",
    "\n",
    "test = {'inputs': X_test, 'targets': y_test}\n",
    "loss, mean_vals, std_vals = nntrainer.test_model(sess, test, batch_size=128, name='test', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = nnmodel.get_parameters(sess, layer='conv1d_0')[0]\n",
    "W = np.squeeze(W.transpose([3, 2, 0, 1]))\n",
    "fig, plt = visualize.plot_filter_logos(W, normalize=True, figsize=(100,50), height=25, \n",
    "                            nt_width=10, norm=0, alphabet='dna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sequence to perform saliency analysis\n",
    "index = 1\n",
    "X = np.expand_dims(X_test[index], axis=0)\n",
    "\n",
    "# parameters for saliency analysis\n",
    "params = {'genome_model': model, \n",
    "          'input_shape': input_shape, \n",
    "          'output_shape': output_shape, \n",
    "          'model_path': file_path+'_best.ckpt',\n",
    "          'optimization': optimization\n",
    "         }\n",
    "\n",
    "# backprop saliency\n",
    "backprop_saliency = saliency.backprop(X, layer='output', class_index=0, params=params)\n",
    "\n",
    "# guided backprop saliency\n",
    "guided_saliency = saliency.guided_backprop(X, layer='output', class_index=0, params=params)\n",
    "\n",
    "# stochastic guided backprop saliency\n",
    "stochastic_saliency = saliency.stochastic_guided_backprop(X, layer='output', class_index=0, params=params,\n",
    "                                                         num_average=200, threshold=1.0, stochastic_val=0.5)\n",
    "\n",
    "# plot saliency comparison\n",
    "fig, ax = plt.subplots(nrows=4, ncols=1, sharex=True, sharey=True, figsize=(15, 3));\n",
    "plt.subplot(4,1,1)\n",
    "visualize.plot_pos_saliency(np.squeeze(backprop_saliency[0]).T)    \n",
    "plt.ylabel(' backprop', fontsize=6)\n",
    "plt.subplot(4,1,2)\n",
    "visualize.plot_pos_saliency(np.squeeze(guided_saliency[0]).T)    \n",
    "plt.ylabel(' guided', fontsize=6)\n",
    "plt.subplot(4,1,3)\n",
    "visualize.plot_pos_saliency(np.squeeze(stochastic_saliency[0]).T)    \n",
    "plt.ylabel(' stochastic', fontsize=6)\n",
    "plt.subplot(4,1,4)\n",
    "visualize.plot_pos_saliency(np.squeeze(X).T)    \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
