{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "from tfomics import neuralnetwork_dev as nn\n",
    "from tfomics import neuralbuild as nb\n",
    "from tfomics import utils, learn, init\n",
    "\n",
    "# import models\n",
    "#from model_zoo import test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 991 ms, sys: 3.21 s, total: 4.2 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "#data_path = 'D:/Dropbox/TFconnect'\n",
    "data_path = '/Users/juliankimura/Desktop/tensorflow/data'\n",
    "filepath = os.path.join(data_path,filename)\n",
    "\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(filepath,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "X_train = dtf[:80000,:,:,:]\n",
    "y_train = ltf[:80000,:]\n",
    "X_valid = dtf_crossval[:1000,:,:,:]\n",
    "y_valid = ltf_crossval[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def model(input_shape, num_labels=None):\n",
    "\n",
    "    # create model\n",
    "    layer1 = {'layer': 'input',\n",
    "            'input_shape': input_shape,\n",
    "            'name': 'input'\n",
    "            }\n",
    "    layer2 = {'layer': 'conv2d', \n",
    "            'num_filters': 16,\n",
    "            'filter_size': (2, 5),\n",
    "            'norm': 'batch',\n",
    "            'activation': 'relu',\n",
    "            'dropout': 0.1,\n",
    "            'name': 'conv1'\n",
    "            }\n",
    "    layer4 = {'layer': 'conv2d', \n",
    "            'num_filters': 32,\n",
    "            'filter_size': (2, 5),\n",
    "            'norm': 'batch',\n",
    "            'activation': 'relu',\n",
    "            'dropout': 0.1,\n",
    "            'name': 'conv2'\n",
    "            }\n",
    "    layer6 = {'layer': 'conv2d', \n",
    "            'num_filters': 10,\n",
    "            'filter_size': (1,1),\n",
    "            'norm': 'batch',\n",
    "            'activation': 'relu',\n",
    "            'dropout': 0.1,\n",
    "            'name': 'conv3'\n",
    "            }\n",
    "    layer7 = {'layer': 'dense', \n",
    "            'num_units': 64,\n",
    "            'norm': 'batch',\n",
    "            'activation': 'relu',\n",
    "            'dropout': 0.2,\n",
    "            'name': 'dense1'\n",
    "            }  \n",
    "    layer9 = {'layer': 'dense', \n",
    "            'num_units': num_labels,\n",
    "            'activation': 'softmax',\n",
    "            'name': 'dense2'\n",
    "            }\n",
    "\n",
    "    #from tfomics import build_network\n",
    "    model_layers = [layer1, layer2, layer4, layer6, layer7, layer9]\n",
    "\n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"categorical\",\n",
    "                  \"optimizer\": \"adam\",\n",
    "                  \"learning_rate\": 0.001,      \n",
    "                  \"l2\": 1e-6,\n",
    "                  # \"l1\": 0, \n",
    "                  }\n",
    "    return model_layers, optimization\n",
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "model_layers, optimization = model(input_shape, num_labels)\n",
    "nnbuild = nb.NeuralBuild(model_layers)\n",
    "network, placeholders, hidden_feed_dict = nnbuild.get_network_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def model(input_shape, num_labels=None):\n",
    "    # design a neural network model\n",
    "\n",
    "    inputs = utils.placeholder(shape=input_shape, name='input')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    targets = utils.placeholder(shape=(None,num_labels), name='output')\n",
    "\n",
    "    # placeholder dictionary\n",
    "    placeholders = {'inputs': inputs, \n",
    "                  'targets': targets, \n",
    "                  'keep_prob': keep_prob, \n",
    "                  'is_training': is_training}\n",
    "\n",
    "\n",
    "    # create model\n",
    "    layer1 = {'layer': 'input',\n",
    "                        'inputs': inputs,\n",
    "                        'name': 'input'\n",
    "                        }\n",
    "    layer2 = {'layer': 'conv2d', \n",
    "                        'num_filters': 25,\n",
    "                        'filter_size': (19,1),\n",
    "                        'W': init.GlorotUniform(),\n",
    "                        'b': init.Constant(0.1),\n",
    "                        #'batch_norm': is_training,\n",
    "                        'padding': 'SAME',\n",
    "                        'activation': 'relu',\n",
    "                        'pool_size': (40,1),\n",
    "                        'name': 'conv1'\n",
    "                        }\n",
    "    layer3 = {'layer': 'residual-conv2d',\n",
    "                        'filter_size': (5,1),\n",
    "                        'batch_norm': is_training,\n",
    "                        'dropout': keep_prob,\n",
    "                        'pool_size': (40,1),\n",
    "                        'name': 'resid1'\n",
    "                     }\n",
    "    layer4 = {'layer': 'dense', \n",
    "                'num_units': 128,\n",
    "                'activation': 'relu',\n",
    "                'W': init.GlorotUniform(),\n",
    "                'b': init.Constant(0.1),\n",
    "                'dropout': keep_prob,\n",
    "                'name': 'dense1'\n",
    "                }\n",
    "    layer5 = {'layer': 'dense', \n",
    "                'num_units': num_labels,\n",
    "                'W': init.GlorotUniform(),\n",
    "                'b': init.Constant(0.1),\n",
    "                'activation': 'sigmoid',\n",
    "                'name': 'dense2'\n",
    "                }\n",
    "\n",
    "    #from tfomics import build_network\n",
    "    model_layers = [layer1, layer2, layer4, layer5]\n",
    "    net = build_network(model_layers)\n",
    "\n",
    "    # optimization parameters\n",
    "    optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.001,      \n",
    "                    \"l2\": 1e-6,\n",
    "                    # \"l1\": 0, \n",
    "                    }\n",
    "\n",
    "    return net, placeholders, optimization\n",
    "\n",
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "model_layers, optimization = model(input_shape, num_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Network architecture:\n",
      "----------------------------------------------------------------------------\n",
      "layer1: input\n",
      "(?, 3, 330, 1)\n",
      "layer2: conv1\n",
      "(?, 2, 326, 16)\n",
      "layer3: conv1_batch\n",
      "(?, 2, 326, 16)\n",
      "layer4: conv1_active\n",
      "(?, 2, 326, 16)\n",
      "layer5: conv1_dropout\n",
      "(?, 2, 326, 16)\n",
      "layer6: conv2\n",
      "(?, 1, 322, 32)\n",
      "layer7: conv2_batch\n",
      "(?, 1, 322, 32)\n",
      "layer8: conv2_active\n",
      "(?, 1, 322, 32)\n",
      "layer9: conv2_dropout\n",
      "(?, 1, 322, 32)\n",
      "layer10: conv3\n",
      "(?, 1, 322, 10)\n",
      "layer11: conv3_batch\n",
      "(?, 1, 322, 10)\n",
      "layer12: conv3_active\n",
      "(?, 1, 322, 10)\n",
      "layer13: conv3_dropout\n",
      "(?, 1, 322, 10)\n",
      "layer14: dense1\n",
      "(?, 64)\n",
      "layer15: dense1_bias\n",
      "(?, 64)\n",
      "layer16: dense1_batch\n",
      "(?, 64)\n",
      "layer17: dense1_active\n",
      "(?, 64)\n",
      "layer18: dense1_dropout\n",
      "(?, 64)\n",
      "layer19: dense2\n",
      "(?, 2)\n",
      "layer20: dense2_bias\n",
      "(?, 2)\n",
      "layer21: output\n",
      "(?, 2)\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(network, placeholders, hidden_feed_dict)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = 'test'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 50 \n",
      "[=====================         ] 70.5% -- time=32s -- loss=0.92538 -- accuracy=50.56%  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0301d18fe498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m learn.train_minibatch(nntrainer, data, batch_size=200, num_epochs=50, \n\u001b[0;32m----> 5\u001b[0;31m                     patience=10, verbose=2, shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/Users/juliankimura/Desktop/tensorflow/tfomics/tfomics/learn.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(nntrainer, data, batch_size, num_epochs, patience, verbose, shuffle)\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\tshuffle=shuffle)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# test current model with cross-validation data and store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/Desktop/tensorflow/tfomics/tfomics/neuralnetwork_dev.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, feed_X, batch_size, verbose, shuffle)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                         \u001b[0mperformance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = {'inputs': [X_train], 'targets': y_train}\n",
    "valid = {'inputs': [X_valid], 'targets': y_valid}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=200, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_mean, batch_var = tf.nn.moments(, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.identity([1., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ema_op = ema.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
