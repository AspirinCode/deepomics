{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics import utils, learn\n",
    "\n",
    "# import models\n",
    "from model_zoo import fourthplace_connectomics_model\n",
    "from model_zoo import simple_connectomics_model, simple_connectomics_model2\n",
    "from model_zoo import residual_connectomics_model, residual_connectomics_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 452 ms, sys: 1.18 s, total: 1.64 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "#data_path = 'D:/Dropbox/TFconnect'\n",
    "data_path = '/home/peter/Code/tensorflow/data'\n",
    "filepath = os.path.join(data_path,filename)\n",
    "\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(filepath,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "X_train = dtf#[:10000,:,:,:]\n",
    "y_train = ltf#[:10000,:]\n",
    "X_valid = dtf_crossval#[:5000,:,:,:]\n",
    "y_valid = ltf_crossval#[:5000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# fourth place competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = fourthplace_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = 'fourth_place'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=100, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple connectomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = simple_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=200, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple connectomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = simple_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version2'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=100, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=100, \n",
    "                    patience=15, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# residual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_parameters(params):\n",
    "    all_params = []\n",
    "    for param in params:\n",
    "        all_params = tf.concat([all_params, tf.reshape(param, [-1,])], axis=0)\n",
    "    return all_params\n",
    "\n",
    "def get_l1_parameters(net):    \n",
    "    params = []\n",
    "    for layer in net:\n",
    "        if hasattr(net[layer], 'is_l1_regularize'):\n",
    "            if net[layer].is_l1_regularize():\n",
    "                variables = net[layer].get_variable()\n",
    "                if isinstance(variables, list):\n",
    "                    for var in variables:\n",
    "                        params.append(var.get_variable())\n",
    "                else:\n",
    "                    params.append(variables.get_variable())\n",
    "    return merge_parameters(params)\n",
    "\n",
    "get_l1_parameters(nnmodel.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = 'residual'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': .8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1., 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=256, num_epochs=100, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': .8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1., 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=256, num_epochs=100, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=2000, num_epochs=100, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# residual model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version_residual2'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.8, 'keep_prob_dense': 0.5, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=200, \n",
    "                    patience=20, verbose=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.9, 'keep_prob_dense': 0.7, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=500, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.9, 'keep_prob_dense': 0.7, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1500, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.9, 'keep_prob_dense': 0.7, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=2000, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': 0.9, 'keep_prob_dense': 0.9, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=3000, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "\n",
    "# set output file paths\n",
    "results_path = utils.make_directory(data_path, 'results')\n",
    "results_path = utils.make_directory(results_path, 'tfomics')\n",
    "output_name = '1d_version_residual2'\n",
    "filepath = os.path.join(results_path, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "optimization['optimizer'] = 'momentum'\n",
    "#optimization['use_nesterov'] = True\n",
    "optimization['learning_rate'] = 0.0001\n",
    "#optimization['momentum'] = 0.99\n",
    "\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)\n",
    "\n",
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob_conv': .9, 'keep_prob_dense': 0.7, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob_conv': 1.0, 'keep_prob_dense': 1.0, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "learn.train_minibatch(nntrainer, data, batch_size=3000, num_epochs=50, \n",
    "                    patience=10, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validation_score(nntrainer, data_path):\n",
    "\n",
    "    filename = 'valideval_dataset.hdf5'\n",
    "    dataset = h5py.File(os.path.join(data_path,filename),'r')\n",
    "    group_name = ['valid_data']\n",
    "    val_dat = np.array(dataset['/'+group_name[0]+'/vs_valid'])\n",
    "    val_lbl = np.array(dataset['/'+group_name[0]+'/label_valid'])\n",
    "\n",
    "    fragLen = 330\n",
    "    N = 14\n",
    "    avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "    startgap = np.ceil(float(val_dat.shape[1] - fragLen)/N).astype('int')\n",
    "    true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "\n",
    "    # Counter for the \"true_lbl\" array\n",
    "    cnt_ = 0\n",
    "    # Counter for the \"pred_lbl\" array\n",
    "    cnt_u = 0\n",
    "    for a in range(val_dat.shape[0]):\n",
    "        if a%100 == 0:\n",
    "            print('\\r' + 'X'*(a//100))\n",
    "\n",
    "        # Create batch array to send thru network\n",
    "        im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "\n",
    "        # Count the number of traces in each batch\n",
    "        cnt = 0\n",
    "\n",
    "        for b in range(val_dat.shape[0]):\n",
    "\n",
    "            for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "                try:\n",
    "                    im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                         val_dat[b,n:n+fragLen],\n",
    "                                         avg_F[n:n+fragLen]))\n",
    "                except:\n",
    "                    from IPython.core.debugger import Tracer\n",
    "                    Tracer()()\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "            # Keep track of the true labels\n",
    "            if val_lbl[a,b] == 1:\n",
    "                true_lbl[cnt_] = 1\n",
    "            else:\n",
    "                true_lbl[cnt_] = 0\n",
    "\n",
    "            cnt_ += 1\n",
    "\n",
    "        # Run batch through network\n",
    "        test = {'inputs': im_eval, 'keep_prob_conv': 1, 'keep_prob_dense': 1, 'is_training': False}\n",
    "        pred_stop = nntrainer.get_activations(test, layer='output')[:,0]\n",
    "        # Average output over each group of N traces\n",
    "        for u in range(0, len(pred_stop), N):\n",
    "            pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N])\n",
    "            cnt_u += 1        \n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "    fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "    wrk = auc(fpr, tpr)\n",
    "    print(wrk)\n",
    "    return wrk\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "validation_score(nntrainer, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_prediction(nntrainer, data_path):\n",
    "\n",
    "    filename = 'competition_dataset_downsampled.hdf5'\n",
    "    dataset = h5py.File(os.path.join(data_path,filename),'r')\n",
    "    group_name = ['competition_data']\n",
    "    val_dat = np.array(dataset['/'+group_name[0]+'/realval'])\n",
    "    val_lbl = np.array(dataset['/'+group_name[0]+'/realtest'])\n",
    "\n",
    "    fragLen = 330\n",
    "    N = 14\n",
    "    avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "    startgap = np.ceil(float(val_dat.shape[1] - fragLen)/N).astype('int')\n",
    "    true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "\n",
    "    # Counter for the \"true_lbl\" array\n",
    "    cnt_ = 0\n",
    "    # Counter for the \"pred_lbl\" array\n",
    "    cnt_u = 0\n",
    "    for a in range(val_dat.shape[0]):\n",
    "        if a%100 == 0:\n",
    "            print('\\r' + 'X'*(a//100))\n",
    "\n",
    "        # Create batch array to send thru network\n",
    "        im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "\n",
    "        # Count the number of traces in each batch\n",
    "        cnt = 0\n",
    "\n",
    "        for b in range(val_dat.shape[0]):\n",
    "\n",
    "            for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "                try:\n",
    "                    im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                         val_dat[b,n:n+fragLen],\n",
    "                                         avg_F[n:n+fragLen]))\n",
    "                except:\n",
    "                    from IPython.core.debugger import Tracer\n",
    "                    Tracer()()\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "            # Keep track of the true labels\n",
    "            if val_lbl[a,b] == 1:\n",
    "                true_lbl[cnt_] = 1\n",
    "            else:\n",
    "                true_lbl[cnt_] = 0\n",
    "\n",
    "            cnt_ += 1\n",
    "\n",
    "        # Run batch through network\n",
    "        test = {'inputs': im_eval, 'keep_prob_conv': 1, 'keep_prob_dense': 1, 'is_training': False}\n",
    "        pred_stop = nntrainer.get_activations(test, layer='output')[:,0]\n",
    "        # Average output over each group of N traces\n",
    "        for u in range(0, len(pred_stop), N):\n",
    "            pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N])\n",
    "            cnt_u += 1        \n",
    "    return pred_lbl\n",
    "\n",
    "\n",
    "nntrainer.set_best_parameters()\n",
    "pred_lbl = test_prediction(nntrainer, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
